{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70fb1423-aaa3-4e44-81e3-54bba947220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def parse_summary(summary):\n",
    "    bowler = \"N/A\"\n",
    "    batsman = \"N/A\"\n",
    "    runs = 0\n",
    "    is_wicket = False\n",
    "    bowler_score = 0  \n",
    "    \n",
    "    try:\n",
    "        if \" to \" in summary:\n",
    "            parts = summary.split(\" to \", 1)\n",
    "            bowler = parts[0].strip()\n",
    "            rest = parts[1].strip()\n",
    "            \n",
    "            if \",\" in rest:\n",
    "                subparts = rest.split(\",\", 1)\n",
    "                batsman = subparts[0].strip()\n",
    "                outcome = subparts[1].strip()\n",
    "                \n",
    "                if \"OUT\" in outcome or \"WICKET\" in outcome:\n",
    "                    runs = 0\n",
    "                    is_wicket = True\n",
    "                    bowler_score = 5  # +5 for a wicket\n",
    "                elif outcome == \"no run\":\n",
    "                    runs = 0\n",
    "                    bowler_score = 1  # +1 for no run\n",
    "                elif \"FOUR\" in outcome or \"4 run\" in outcome:\n",
    "                    runs = 4\n",
    "                    bowler_score = -2  # -2 for a four\n",
    "                elif \"SIX\" in outcome or \"6 run\" in outcome:\n",
    "                    runs = 6\n",
    "                    bowler_score = -3  # -3 for a six\n",
    "                elif \"leg bye\" in outcome or \"bye\" in outcome:\n",
    "                    if \"leg bye\" in outcome:\n",
    "                        match = re.search(r'(\\d+) leg bye', outcome)\n",
    "                        runs = int(match.group(1)) if match else 1\n",
    "                    else:  # regular bye\n",
    "                        match = re.search(r'(\\d+) bye', outcome)\n",
    "                        runs = int(match.group(1)) if match else 1\n",
    "                    bowler_score = -0.25  # -0.25 for leg bye or bye\n",
    "                elif \"wide\" in outcome:\n",
    "                    # Wides are always at least 1 run\n",
    "                    match = re.search(r'(\\d+) wide', outcome)\n",
    "                    runs = int(match.group(1)) if match else 1\n",
    "                    bowler_score = -1  # -1 for wide\n",
    "                elif \"no ball\" in outcome:\n",
    "                    # No balls are 1 run plus any runs scored\n",
    "                    runs = 1  # Start with 1 for the no ball\n",
    "                    \n",
    "                    # Check if additional runs were scored\n",
    "                    run_match = re.search(r'(\\d+) run', outcome)\n",
    "                    if run_match:\n",
    "                        runs += int(run_match.group(1))\n",
    "                    bowler_score = -1  # -1 for no ball (similar to wide)\n",
    "                # Regular runs\n",
    "                else:\n",
    "                    match = re.search(r'(\\d+) run', outcome)\n",
    "                    if match:\n",
    "                        runs = int(match.group(1))\n",
    "                        # For regular runs (not 0, 4, or 6)\n",
    "                        if runs > 0:\n",
    "                            bowler_score = -runs / 2  # Negative score proportional to runs\n",
    "        \n",
    "        return bowler, batsman, runs, is_wicket, bowler_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing summary: {e}\")\n",
    "        return \"N/A\", \"N/A\", 0, False, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b4a1920-be67-46fa-b4f6-e89aec7c6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_data(df):\n",
    "    if df[\"Ball Number\"].dtype == 'object':\n",
    "        df[[\"over_str\", \"ball_str\"]] = df[\"Ball Number\"].str.split(\".\", expand=True)\n",
    "        df[\"over\"] = pd.to_numeric(df[\"over_str\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "        df[\"ball\"] = pd.to_numeric(df[\"ball_str\"], errors=\"coerce\").fillna(-1).astype(int)\n",
    "        \n",
    "        df[\"sort_order\"] = df[\"over\"] * 6 + df[\"ball\"]\n",
    "        df = df.sort_values(by=[\"sort_order\"]).reset_index(drop=True)\n",
    "        df = df.drop(columns=[\"over_str\", \"ball_str\", \"sort_order\"])\n",
    "    \n",
    "    df[\"Ball\"] = df.index + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "28c3dec8-af4a-4457-b254-b1f7bb2aa540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_commentary_data(page_source):\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    \n",
    "    ball_containers = soup.find_all(\"div\", class_=lambda x: x and \"ds-text-tight-l\" in x and \"ds-flex\" in x)\n",
    "    \n",
    "    ball_numbers = []\n",
    "    summaries = []\n",
    "    descriptions = []\n",
    "    \n",
    "    for container in ball_containers:\n",
    "        ball_span = container.find(\"span\", class_=\"ds-text-tight-s ds-font-regular ds-mb-1 lg:ds-mb-0 lg:ds-mr-3 ds-block ds-text-center ds-text-typo-mid1\")\n",
    "        ball_text = ball_span.get_text(strip=True) if ball_span else \"N/A\"\n",
    "        \n",
    "        summary_div = container.find(\"div\", class_=\"ds-leading-[16px] lg:ds-leading-none ds-mb-0.5\")\n",
    "        summary_text = summary_div.get_text(strip=True) if summary_div else \"N/A\"\n",
    "        \n",
    "        desc_p = container.find(\"p\", class_=\"ci-html-content first-letter:ds-capitalize ds-leading-[24px]\")\n",
    "        desc_text = desc_p.get_text(strip=True) if desc_p else \"N/A\"\n",
    "        \n",
    "        ball_numbers.append(ball_text)\n",
    "        summaries.append(summary_text)\n",
    "        descriptions.append(desc_text)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        \"Ball Number\": ball_numbers,\n",
    "        \"Summary\": summaries,\n",
    "        \"Description\": descriptions\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "328e67c5-0750-4e90-b890-f086795a5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_match_data(df, match_id, target, city, stadium, player_names):\n",
    "    # Function to find the full name from player_names set\n",
    "    def get_full_name(short_name):\n",
    "        # First check if the short name itself is in player_names\n",
    "        if short_name in player_names:\n",
    "            return short_name\n",
    "        \n",
    "        # Look for full names containing this short name\n",
    "        for full_name in player_names:\n",
    "            # Split full name into parts and check if any part matches the short name\n",
    "            name_parts = full_name.split()\n",
    "            if short_name in name_parts:\n",
    "                return full_name\n",
    "        \n",
    "        # If no match found, return the original name\n",
    "        return short_name\n",
    "    \n",
    "    # Apply the parse_summary function to extract data from the summary\n",
    "    parsed_data = df[\"Summary\"].apply(parse_summary)\n",
    "    df['bowler'], df['batsman'], df['Runs'], df['is_wicket'], df['bowler_score'] = zip(*parsed_data)\n",
    "    \n",
    "    # Replace shortened names with full names from player_names set\n",
    "    df['bowler'] = df['bowler'].apply(get_full_name)\n",
    "    df['batsman'] = df['batsman'].apply(get_full_name)\n",
    "    \n",
    "    # Evaluate bowler performance for each ball\n",
    "    df['bowler_good'] = df.apply(lambda row: 1 if row['bowler_score'] > 0 else 0, axis=1)\n",
    "    \n",
    "    # Ensure Runs column is numeric\n",
    "    df['Runs'] = pd.to_numeric(df['Runs'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Add metadata\n",
    "    df['match_id'] = match_id\n",
    "    df['city'] = city\n",
    "    df['stadium'] = stadium\n",
    "    \n",
    "    # Identify extras and calculate legal deliveries\n",
    "    df['is_extra'] = df['Summary'].str.contains(r'wide|no ball', case=False, regex=True)\n",
    "    df['legal_delivery'] = ~df['is_extra']\n",
    "    df['legal_deliveries_count'] = df['legal_delivery'].cumsum()\n",
    "    \n",
    "    # Calculate overs properly based on legal deliveries\n",
    "    df['overs_completed'] = (df['legal_deliveries_count'] - 1) // 6\n",
    "    df['balls_in_current_over'] = ((df['legal_deliveries_count'] - 1) % 6) + 1\n",
    "    \n",
    "    # Handle the case where legal_deliveries_count is 0 (first ball)\n",
    "    df.loc[df['legal_deliveries_count'] == 0, 'overs_completed'] = 0\n",
    "    df.loc[df['legal_deliveries_count'] == 0, 'balls_in_current_over'] = 0\n",
    "    \n",
    "    # Calculate decimal overs\n",
    "    df['overs_bowled'] = df['overs_completed'] + (df['balls_in_current_over'] / 6)\n",
    "    df['overs_completed'] = df['overs_completed'].clip(lower=0)\n",
    "    df['overs_bowled'] = df['overs_bowled'].clip(lower=0)\n",
    "    \n",
    "    # Calculate run statistics\n",
    "    df['total_runs'] = df['Runs'].cumsum()\n",
    "    df['current_run_rate'] = df.apply(\n",
    "        lambda row: row['total_runs'] / max(row['overs_bowled'], 0.1) if row['overs_bowled'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Add match metadata and calculate target-related stats\n",
    "    total_overs = 20  # Standard T20 match\n",
    "    df['total_overs'] = total_overs\n",
    "    df['target'] = target\n",
    "    df['runs_needed'] = np.maximum(target - df['total_runs'], 0)\n",
    "    df['overs_remaining'] = np.maximum(df['total_overs'] - df['overs_bowled'], 0)\n",
    "    \n",
    "    # Calculate required run rate\n",
    "    df['required_run_rate'] = df.apply(\n",
    "        lambda row: row['runs_needed'] / np.maximum(row['overs_remaining'], 0.1) \n",
    "                    if row['overs_remaining'] > 0 else float('inf'),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Calculate wicket statistics\n",
    "    df['wickets_fallen'] = df['is_wicket'].cumsum()\n",
    "    df['wickets_in_hand'] = 10 - df['wickets_fallen']\n",
    "    \n",
    "    # Clean up unnecessary columns\n",
    "    df = df.drop(columns=['Summary', 'balls_in_current_over', 'Description', 'bowler_good'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8b8638e-e01d-4f4b-9324-a54959938ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import urllib3, socket\n",
    "from urllib3.connection import HTTPConnection\n",
    "    \n",
    "def extraction2(url, target, id, city, stadium, player_names):\n",
    "    match_id = id\n",
    "    HTTPConnection.default_socket_options = ( \n",
    "            HTTPConnection.default_socket_options + [\n",
    "            (socket.SOL_SOCKET, socket.SO_SNDBUF, 1000000), #1MB in byte\n",
    "            (socket.SOL_SOCKET, socket.SO_RCVBUF, 1000000)\n",
    "        ])\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    #chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.set_page_load_timeout(300)  # Increased timeout\n",
    "    driver.set_script_timeout(300)     # Increased timeout\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(driver, 60).until(  # Increased wait time\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.ds-text-tight-l.ds-flex\"))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error waiting for page to load: {e}\")\n",
    "            driver.quit()\n",
    "            return None\n",
    "        \n",
    "        scroll_pause_time = 1\n",
    "        \n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollBy({ top: 200, behavior: 'smooth' });\")\n",
    "            time.sleep(0.2) \n",
    "                    \n",
    "            # Check for the presence of \"0.1\" in ball numbers\n",
    "            ball_spans = driver.find_elements(By.CSS_SELECTOR, \"span.ds-text-tight-s.ds-font-regular.ds-mb-1.lg\\\\:ds-mb-0.lg\\\\:ds-mr-3.ds-block.ds-text-center.ds-text-typo-mid1\")\n",
    "            ball_texts = [span.text.strip() for span in ball_spans if span.text.strip()]\n",
    "            if \"0.1\" in ball_texts:\n",
    "                break\n",
    "        \n",
    "        \n",
    "        time.sleep(2)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            print(\"Error closing browser\")\n",
    "\n",
    "    \n",
    "    df = extract_commentary_data(page_source)\n",
    "    df.to_csv(\"cricket_commentary_raw.csv\", index=False)\n",
    "    \n",
    "    try:\n",
    "        df = preprocess_raw_data(df)\n",
    "        df.to_csv(\"commentary_processed.csv\", index=False)\n",
    "        \n",
    "        df = analyze_match_data(df, int(id), int(target), city, stadium, player_names)\n",
    "        \n",
    "        # Save the final processed commentary CSV\n",
    "        df.to_csv(\"processed_cricket_commentary.csv\", index=False)\n",
    "        print(\"Final processed dataset saved as 'processed_cricket_commentary.csv'.\")\n",
    "        \n",
    "        # Save the enhanced dataset with metadata calculations\n",
    "        filename = f\"{match_id}b.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Dataset saved as '{filename}'.\")\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing cricket commentary: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fdbdc8c2-9df5-4f10-8bb8-5536615fba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import urllib3, socket\n",
    "from urllib3.connection import HTTPConnection\n",
    "\n",
    "def extraction1(url, target, id, city, stadium, player_names):\n",
    "\n",
    "    match_id = id\n",
    "    HTTPConnection.default_socket_options = ( \n",
    "            HTTPConnection.default_socket_options + [\n",
    "            (socket.SOL_SOCKET, socket.SO_SNDBUF, 1000000), #1MB in byte\n",
    "            (socket.SOL_SOCKET, socket.SO_RCVBUF, 1000000)\n",
    "        ])\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.set_page_load_timeout(300)  # Increased timeout\n",
    "    driver.set_script_timeout(300)     # Increased timeout\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait for the page to load using a more general selector\n",
    "        try:\n",
    "            WebDriverWait(driver, 60).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.ds-text-tight-l.ds-flex\"))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error waiting for page to load: {e}\")\n",
    "            driver.quit()\n",
    "            return None\n",
    "            \n",
    "        # Improved approach for innings selection\n",
    "        try:\n",
    "            # First wait for the page to stabilize\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Use a more general approach to find the dropdown\n",
    "            dropdowns = WebDriverWait(driver, 20).until(\n",
    "                EC.presence_of_all_elements_located((By.CLASS_NAME, \"ds-popper-wrapper\"))\n",
    "            )\n",
    "            \n",
    "            if len(dropdowns) > 1:\n",
    "                # Use JavaScript to click, which is more reliable\n",
    "                driver.execute_script(\"arguments[0].click();\", dropdowns[1])\n",
    "                \n",
    "                # Wait for dropdown menu to appear\n",
    "                WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, \"//div[@data-tippy-root]\"))\n",
    "                )\n",
    "                \n",
    "                # Find all innings options with a more reliable selector\n",
    "                innings_options = driver.find_elements(By.CSS_SELECTOR, \"div[data-tippy-root] li.ds-w-full.ds-flex\")\n",
    "                \n",
    "                if innings_options and len(innings_options) >= 1:\n",
    "                    # Click the first innings using JavaScript\n",
    "                    driver.execute_script(\"arguments[0].click();\", innings_options[0])\n",
    "                    \n",
    "                    # Wait for content to refresh after selection\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    print(\"No innings options found or accessible.\")\n",
    "            else:\n",
    "                print(\"Dropdown elements not found as expected.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error selecting innings: {e}\")\n",
    "            # Continue with the script even if innings selection fails\n",
    "        \n",
    "        # Original scroll code with minor improvements\n",
    "        scroll_pause_time = 1\n",
    "        max_scroll_attempts = 30  # Limit scrolling attempts\n",
    "        scroll_attempts = 0\n",
    "        \n",
    "        while scroll_attempts < max_scroll_attempts:\n",
    "            driver.execute_script(\"window.scrollBy({ top: 200, behavior: 'smooth' });\")\n",
    "            time.sleep(0.5)  # Slightly longer pause for stability\n",
    "            \n",
    "            # Check for the presence of \"0.1\" in ball numbers\n",
    "            ball_spans = driver.find_elements(By.CSS_SELECTOR, \"span.ds-text-tight-s.ds-font-regular.ds-mb-1.lg\\\\:ds-mb-0.lg\\\\:ds-mr-3.ds-block.ds-text-center.ds-text-typo-mid1\")\n",
    "            ball_texts = [span.text.strip() for span in ball_spans if span.text.strip()]\n",
    "            if \"0.1\" in ball_texts:\n",
    "                break\n",
    "                \n",
    "            scroll_attempts += 1\n",
    "        \n",
    "        time.sleep(2)\n",
    "        page_source = driver.page_source\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "        return None\n",
    "    finally:\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            print(\"Error closing browser\")\n",
    "    \n",
    "    df = extract_commentary_data(page_source)\n",
    "    df.to_csv(\"cricket_commentary_raw.csv\", index=False)\n",
    "    \n",
    "    try:\n",
    "        df = preprocess_raw_data(df)\n",
    "        df.to_csv(\"commentary_processed.csv\", index=False)\n",
    "        \n",
    "        df = analyze_match_data(df, int(id), int(target), city, stadium, player_names)\n",
    "        \n",
    "        # Save the final processed commentary CSV\n",
    "        df.to_csv(\"processed_cricket_commentary.csv\", index=False)\n",
    "        \n",
    "        # Save the enhanced dataset with metadata calculations\n",
    "        filename = f\"{match_id}a.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Dataset saved as '{filename}'.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing cricket commentary: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d4bb031-3518-4176-a44d-693a22a83de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def pre_start(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Ensure page is fully loaded before interacting\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "        \n",
    "        # Implement a more robust way to scroll\n",
    "        # Scroll multiple times with pauses to ensure all content loads\n",
    "        scroll_pause_time = 2\n",
    "        screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "        i = 1\n",
    "        \n",
    "        while True:\n",
    "            # Scroll one screen at a time\n",
    "            driver.execute_script(f\"window.scrollTo(0, {screen_height * i});\")\n",
    "            i += 1\n",
    "            time.sleep(scroll_pause_time)\n",
    "            \n",
    "            # Check if we've reached the bottom\n",
    "            scroll_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "            if (screen_height * i) > scroll_height:\n",
    "                break\n",
    "                \n",
    "        # Wait additional time for all dynamic content to load\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # Re-fetch page source after scrolling\n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Fetch stadium info\n",
    "        stadium_info = soup.find(\"a\", href=re.compile(\"/cricket-grounds/\"))\n",
    "        if stadium_info:\n",
    "            title_parts = stadium_info[\"title\"].split(\", \")\n",
    "            if len(title_parts) >= 2:\n",
    "                stadium_name, city = title_parts[0], title_parts[1]\n",
    "            else:\n",
    "                stadium_name, city = title_parts[0], \"NA\"\n",
    "        else:\n",
    "            print(\"Stadium information not found.\")\n",
    "            stadium_name, city = \"NA\", \"NA\"\n",
    "            \n",
    "        # Re-fetch tables after waiting\n",
    "        tables = soup.find_all('table')\n",
    "        if len(tables) > 4:\n",
    "            tables = tables[:4]\n",
    "            \n",
    "        # Set for player names\n",
    "        player_names = set()\n",
    "        \n",
    "        # Regex pattern to remove (c), †, and any text inside parentheses\n",
    "        pattern = r\"\\s*\\([^)]*\\)|†\"\n",
    "        \n",
    "        # Extract names from tables\n",
    "        for table in tables:\n",
    "            tbody = table.find('tbody')\n",
    "            if tbody:\n",
    "                rows = tbody.find_all('tr')\n",
    "                for row in rows:\n",
    "                    player_cell = row.find('td')\n",
    "                    if player_cell:\n",
    "                        name_span = player_cell.find('span', class_='ds-text-tight-s')\n",
    "                        if name_span:\n",
    "                            clean_name = re.sub(pattern, \"\", name_span.text).strip()\n",
    "                            if clean_name:\n",
    "                                player_names.add(clean_name)\n",
    "        \n",
    "        return stadium_name, city, player_names\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in pre_start: {e}\")\n",
    "        return \"NA\", \"NA\", set()\n",
    "    \n",
    "    finally:\n",
    "        driver.quit() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a089ff3-48b4-4760-b913-ff13d269557e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the script stopped previously, you can resume from a specific match.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the match number to start from (or press Enter to start from 1):  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping from match number 5\n",
      "Found 74 match containers.\n",
      "\n",
      "Processing match 5...\n",
      "Fetching data from scorecard: https://www.espncricinfo.com/series/indian-premier-league-2023-1345038/royal-challengers-bangalore-vs-mumbai-indians-5th-match-1359479/full-scorecard\n",
      "First innings Done\n",
      "An error occurred: Message: stale element reference: stale element not found in the current frame\n",
      "  (Session info: chrome=134.0.6998.35); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#stale-element-reference-exception\n",
      "Stacktrace:\n",
      "#0 0x5fd71f88cffa <unknown>\n",
      "#1 0x5fd71f34b970 <unknown>\n",
      "#2 0x5fd71f35250e <unknown>\n",
      "#3 0x5fd71f354d58 <unknown>\n",
      "#4 0x5fd71f354de3 <unknown>\n",
      "#5 0x5fd71f39799d <unknown>\n",
      "#6 0x5fd71f3c3292 <unknown>\n",
      "#7 0x5fd71f390e4a <unknown>\n",
      "#8 0x5fd71f3c345e <unknown>\n",
      "#9 0x5fd71f3e970c <unknown>\n",
      "#10 0x5fd71f3c3063 <unknown>\n",
      "#11 0x5fd71f38f328 <unknown>\n",
      "#12 0x5fd71f390491 <unknown>\n",
      "#13 0x5fd71f85442b <unknown>\n",
      "#14 0x5fd71f8582ec <unknown>\n",
      "#15 0x5fd71f83ba22 <unknown>\n",
      "#16 0x5fd71f858e64 <unknown>\n",
      "#17 0x5fd71f81fbef <unknown>\n",
      "#18 0x5fd71f87b558 <unknown>\n",
      "#19 0x5fd71f87b736 <unknown>\n",
      "#20 0x5fd71f88be76 <unknown>\n",
      "#21 0x715d81a9caa4 <unknown>\n",
      "#22 0x715d81b29c3c <unknown>\n",
      "\n",
      "Second innings Done\n",
      "Closing main browser...\n",
      "\n",
      "Data extraction complete.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 133\u001b[0m\n\u001b[1;32m    130\u001b[0m start_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(user_input) \u001b[38;5;28;01mif\u001b[39;00m user_input \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting scraping from match number \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_match\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m match_details \u001b[38;5;241m=\u001b[39m get_target_and_commentary_urls(schedule_url, start_match)\n",
      "Cell \u001b[0;32mIn[50], line 105\u001b[0m, in \u001b[0;36mget_target_and_commentary_urls\u001b[0;34m(url, start_match)\u001b[0m\n\u001b[1;32m    103\u001b[0m     extraction1(commentary_url, target, idx, city, stadium, player_names)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSecond innings Done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 105\u001b[0m     extraction2(commentary_url, target, idx, city, stadium, player_names)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing match \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[46], line 47\u001b[0m, in \u001b[0;36mextraction2\u001b[0;34m(url, target, id, city, stadium, player_names)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollBy(\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m top: 200, behavior: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmooth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m });\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 47\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.2\u001b[39m) \n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Check for the presence of \"0.1\" in ball numbers\u001b[39;00m\n\u001b[1;32m     50\u001b[0m     ball_spans \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan.ds-text-tight-s.ds-font-regular.ds-mb-1.lg\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m:ds-mb-0.lg\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m:ds-mr-3.ds-block.ds-text-center.ds-text-typo-mid1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "def get_target_and_commentary_urls(url, start_match=1):\n",
    "    driver = webdriver.Chrome()\n",
    "    extracted_data = []\n",
    "    match_id = start_match\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Increase wait time for initial page load\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"ds-p-4\"))\n",
    "        )\n",
    "        \n",
    "        # Scroll down to load all matches\n",
    "        scroll_pause_time = 2\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(scroll_pause_time)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "        \n",
    "        # Get fresh page source after all content is loaded\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        matches = soup.find_all('div', class_='ds-p-4 ds-border-y ds-border-line')\n",
    "        print(f\"Found {len(matches)} match containers.\")\n",
    "        driver.quit()\n",
    "        for idx, match in enumerate(matches[start_match-1:], start=start_match):\n",
    "            print(f\"\\nProcessing match {idx}...\")\n",
    "            try:\n",
    "                link_tag = match.find('a', href=True)\n",
    "                if not link_tag:\n",
    "                    print(\"No link tag found; skipping this match.\")\n",
    "                    match_id += 1\n",
    "                    continue\n",
    "                    \n",
    "                original_url = 'https://www.espncricinfo.com' + link_tag['href']\n",
    "                \n",
    "                # Get scorecard URL and fetch preprocessing data\n",
    "                url_parts1 = original_url.split('/')\n",
    "                url_parts1[-1] = 'full-scorecard'\n",
    "                scorecard_url = '/'.join(url_parts1)\n",
    "                \n",
    "                print(f\"Fetching data from scorecard: {scorecard_url}\")\n",
    "                stadium, city, player_names = pre_start(scorecard_url)\n",
    "                \n",
    "                # Get commentary URL\n",
    "                url_parts = original_url.split('/')\n",
    "                url_parts[-1] = 'ball-by-ball-commentary'\n",
    "                commentary_url = '/'.join(url_parts)\n",
    "                \n",
    "                # Extract city from the original match data\n",
    "                local_city = None\n",
    "                details_div = match.find('div', class_='ds-text-tight-s ds-font-regular ds-truncate ds-text-typo-mid3')\n",
    "                if details_div:\n",
    "                    span = details_div.find('span', class_='ds-text-tight-s ds-font-medium ds-text-typo')\n",
    "                    if span:\n",
    "                        next_text = span.next_sibling\n",
    "                        if next_text and isinstance(next_text, str):\n",
    "                            city_part = next_text.strip().lstrip('•').strip()\n",
    "                            local_city = city_part.split(',')[0].strip()\n",
    "                \n",
    "                # Use the city from scorecard if available, otherwise use the one from match page\n",
    "                if city == \"NA\" and local_city:\n",
    "                    city = local_city\n",
    "                \n",
    "                # Extract target\n",
    "                target = None\n",
    "                team_scores = match.find_all('div', class_='ci-team-score')\n",
    "                for score_div in team_scores:\n",
    "                    score_text_div = score_div.find('div', class_='ds-text-compact-s ds-text-typo ds-text-right ds-whitespace-nowrap')\n",
    "                    if score_text_div:\n",
    "                        target_span = score_text_div.find('span', class_='ds-text-compact-xs ds-mr-0.5')\n",
    "                        if target_span and target_span.text.strip():\n",
    "                            target_match = re.search(r'T:(\\d+)', target_span.text)\n",
    "                            if target_match:\n",
    "                                target = int(target_match.group(1))\n",
    "                                break\n",
    "                \n",
    "                if target is None:\n",
    "                    target = 0\n",
    "                    print(\"No target score found for this match.\")\n",
    "                \n",
    "                # Append all data\n",
    "                extracted_data.append({\n",
    "                    'match_id': idx,\n",
    "                    'city': city,\n",
    "                    'stadium': stadium,\n",
    "                    'target_score': target,\n",
    "                    'commentary_url': commentary_url,\n",
    "                    'player_names': player_names\n",
    "                })\n",
    "                \n",
    "                # Call extraction functions with fresh data\n",
    "                print(\"First innings Done\")\n",
    "                extraction1(commentary_url, target, idx, city, stadium, player_names)\n",
    "                print(\"Second innings Done\")\n",
    "                extraction2(commentary_url, target, idx, city, stadium, player_names)\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing match {idx}: {e}\")\n",
    "                            \n",
    "                \n",
    "            # Wait between match processing to avoid overloading\n",
    "            time.sleep(3)\n",
    "        \n",
    "        return extracted_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main process: {e}\")\n",
    "        return extracted_data  \n",
    "    \n",
    "    finally:\n",
    "        print(\"Closing main browser...\")\n",
    "        print(\"\\nData extraction complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    schedule_url = \"https://www.espncricinfo.com/series/indian-premier-league-2023-1345038/match-schedule-fixtures-and-results\"\n",
    "    print(\"If the script stopped previously, you can resume from a specific match.\")\n",
    "    user_input = input(\"Enter the match number to start from (or press Enter to start from 1): \")        \n",
    "    start_match = int(user_input) if user_input else 1\n",
    "    \n",
    "    print(f\"Starting scraping from match number {start_match}\")\n",
    "    match_details = get_target_and_commentary_urls(schedule_url, start_match)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab224b1-493a-47c0-95c9-80e05e297622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
